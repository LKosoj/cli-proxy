# agent/plugins/web_research

Модуль WebResearchTool предназначен для выполнения семантического поиска релевантных веб-статей на основе пользовательского запроса. Поддерживает мультиязычный поиск (русский, английский, китайский), генерацию уточнённых поисковых запросов с помощью OpenAI, поиск через Jina AI Search API, а также загрузку, очистку и анализ содержимого страниц. Реализованы механизмы обработки ошибок, дедупликации и fallback-перехода между методами извлечения контента.

Ключевые структуры данных  
LANG_QUERIES_PROMPTS — Словарь с языковыми шаблонами для генерации поисковых запросов через LLM

---
WebResearchTool.get_source_name() -> str  
Возвращает имя источника инструмента.  
Возвращает  
Имя источника — "Web Research"

---
WebResearchTool.get_spec() -> ToolSpec  
Возвращает спецификацию инструмента, включая параметры и описание.  
Возвращает  
Объект ToolSpec с описанием функциональности и параметров инструмента

---
WebResearchTool._get_jina_api_key() -> Optional[str]  
Извлекает API-ключ Jina AI из переменных окружения или конфигурации.  
Возвращает  
API-ключ Jina AI или None, если не найден

---
WebResearchTool._get_openai_client() -> AsyncOpenAI  
Создаёт асинхронного клиента OpenAI с использованием API-ключа и базового URL из окружения или конфигурации.  
Возвращает  
Экземпляр AsyncOpenAI

---
WebResearchTool._get_model(big: bool = False) -> str  
Определяет модель OpenAI, используемую для генерации запросов или анализа.  
Аргументы  
big — Если True, возвращает более мощную модель (например, gpt-4o), иначе — лёгкую (например, gpt-4o-mini)  
Возвращает  
Название модели для использования

---
WebResearchTool._call_openai_for_queries(user_prompt: str, system_prompt: Optional[str] = None) -> List[str]  
Генерирует список поисковых запросов с помощью OpenAI на основе пользовательского и системного промптов.  
Аргументы  
user_prompt — Основной промпт с запросом от пользователя  
system_prompt — Необязательный системный промпт для настройки поведения модели  
Возвращает  
Список строк — сгенерированных поисковых запросов (до 5)  
Исключения  
Любые исключения логируются и не прерывают выполнение — возвращается пустой список

---
WebResearchTool._jina_search(query: str, max_results: int = 5) -> List[str]  
Выполняет поиск ссылок через Jina AI Search API по заданному запросу.  
Аргументы  
query — Поисковый запрос  
max_results — Максимальное количество возвращаемых результатов (по умолчанию: 5)  
Возвращает  
Список URL-адресов, найденных по запросу  
Исключения  
Логирует ошибки (отсутствие ключа, сетевые ошибки, статусы ответа) — в случае ошибки возвращает пустой список

---
async def _download_content(self, url: str) -> Dict[str, str]  
Асинхронно скачивает и очищает содержимое страницы по указанному URL.  
Аргументы  
url — URL страницы для загрузки и обработки  
Возвращает  
Словарь с полями "url", "title", "content", содержащий обработанный контент или пустые значения при ошибке  
Исключения  
Не выбрасывает исключения напрямую, все ошибки перехватываются и логируются

---
async def _extract_pdf_content(self, pdf_bytes: bytes, url: str) -> Tuple[str, str]  
Извлекает текстовое содержимое и заголовок из PDF-документа.  
Аргументы  
pdf_bytes — Содержимое PDF в виде байтов  
url — Исходный URL PDF для определения заголовка при отсутствии текста  
Возвращает  
Кортеж из извлечённого текста и заголовка (обычно имя файла из URL)  
Исключения  
Exception — При ошибках извлечения текста (кроме ImportError, обрабатывается отдельно)

---
async def _get_clean_text_jina(self, url: str) -> Tuple[str, str]  
Получает очищенный Markdown-контент и заголовок через Jina Reader API.  
Аргументы  
url — URL страницы, контент которой нужно извлечь  
Возвращает  
Кортеж из Markdown-контента и извлечённого заголовка  
Исключения  
Exception — Если Jina API ключ не настроен или не удалось извлечь контент

---
def _extract_title(self, html_content: str) -> str  
Извлекает заголовок страницы из HTML-содержимого с помощью BeautifulSoup.  
Аргументы  
html_content — HTML-код страницы в виде строки  
Возвращает  
Извлечённый заголовок страницы или пустая строка, если не найден

---
def _extract_title_from_html(html_content: str) -> str  
Извлекает заголовок из HTML: сначала из тега <title>, затем из og:title, и, наконец, из первого <h1>.  
Аргументы  
html_content — строка с HTML-содержимым страницы  
Возвращает  
Извлечённый и очищенный заголовок или "Без заголовка" при неудаче  
Исключения  
Любые исключения логируются как предупреждение, не прерывают выполнение

---
def _clean_extra_spaces(text: str) -> str  
Удаляет лишние пробелы и пустые строки, оставляя только значимые строки, разделённые символом новой строки.  
Аргументы  
text — входной текст для очистки  
Возвращает  
Текст без лишних пробелов и пустых строк, с нормализованными переносами

---
def _clean_html_content(html_content: str) -> str  
Очищает HTML-содержимое, удаляя ненужные теги и преобразуя оставшийся текст в чистый формат с разрывами строк.  
Аргументы  
html_content — строка с HTML-содержимым  
Возвращает  
Очищенный текст с разрывами строк после блочных элементов  
Исключения  
При ошибках возвращает исходный HTML и логирует ошибку

---
async def _generate_search_queries_lang(user_query: str, lang: str, n: int) -> List[str]  
Генерирует n поисковых запросов на указанном языке с использованием LLM.  
Аргументы  
user_query — исходный запрос пользователя  
lang — язык запроса (например, "ru", "en")  
n — количество генерируемых запросов  
Возвращает  
Список из n сгенерированных поисковых запросов  
Исключения  
Ошибки вызова модели обрабатываются внутри, возвращается пустой список

---
async def _find_articles_for_language(user_query: str, lang: str, num_queries: int, max_results: int) -> List[str]  
Находит статьи по запросу на заданном языке, выполняя несколько поисковых запросов и объединяя результаты.  
Аргументы  
user_query — исходный запрос пользователя  
lang — язык поиска  
num_queries — количество генерируемых поисковых запросов  
max_results — максимальное общее количество результатов  
Возвращает  
Список уникальных URL-адресов, отсортированных по методу round-robin

---
def _round_robin_merge(lists: List[List[str]]) -> List[str]  
Объединяет несколько списков URL-адресов методом round-robin, исключая дубликаты.  
Аргументы  
lists — список списков URL-адресов  
Возвращает  
Объединённый список URL без дубликатов, с чередованием по одному из каждого списка

---
async def _analyze_content_with_llm(user_query: str, articles: List[Dict[str, str]]) -> str  
Анализирует содержимое собранных статей с помощью большой языковой модели и формирует структурированный ответ.  
Аргументы  
user_query — исходный запрос пользователя  
articles — список словарей с полями "url", "title", "content"  
Возвращает  
Структурированный текстовый ответ с анализом и ссылками на источники  
Исключения  
При ошибках возвращает сообщение об ошибке

---
async def execute(self, args: Dict[str, Any], ctx: Dict[str, Any]) -> Dict[str, Any]  
Основной метод выполнения веб-исследования: от генерации запросов до анализа и возврата результата.  
Аргументы  
args — словарь с параметрами: "query", "max_results_per_lang", "analyze_content"  
ctx — контекст выполнения (не используется напрямую)  
Возвращает  
Словарь с результатом: {"success": bool, "result": str} или {"success": False, "error": str}  
Исключения  
Все исключения перехватываются, ошибка возвращается в поле "error"
