# agent/memory_policy

Модуль отвечает за управление долговременной памятью проекта: принимает решение о сохранении информации на основе диалога и сжимает память при превышении лимита. Использует LLM через `chat_completion` для анализа и обработки текста памяти. Оперирует строго структурированными данными и поддерживает категории важности для фильтрации содержимого.

Ключевые структуры данных  
AppConfig — Конфигурация приложения, используемая для настройки клиента OpenAI

---
async def decide_memory_save(config: AppConfig, user_text: str, final_response: str, memory_text: str) -> Optional[Tuple[str, str]]  
Принимает решение, нужно ли сохранить информацию в долговременную память, и возвращает тег и содержание записи.  
Аргументы  
config — Конфигурация приложения для доступа к OpenAI  
user_text — Текст запроса пользователя  
final_response — Финальный ответ системы  
memory_text — Текущее содержимое памяти в текстовом виде  
Возвращает  
Кортеж из тега (например, "PREF") и содержания записи, если решение — сохранить; иначе None  
Исключения  
Нет явных исключений; ошибки парсинга JSON подавляются, возвращается None

---
async def compress_memory(config: AppConfig, memory_text: str, max_chars: int) -> Optional[str]  
Сжимает текст памяти до заданного количества символов, сохраняя наиболее важные и устойчивые записи.  
Аргументы  
config — Конфигурация приложения для доступа к OpenAI  
memory_text — Текущее содержимое памяти  
max_chars — Максимально допустимое количество символов в сжатой памяти  
Возвращает  
Сжатое содержимое памяти в виде строки или None при ошибке  
Исключения  
Нет явных исключений; ошибки взаимодействия с API обрабатываются внутри chat_completion
